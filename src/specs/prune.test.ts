// src/specs/prune.test.ts
import { config } from 'dotenv';
config();
import {
  HumanMessage,
  AIMessage,
  SystemMessage,
  BaseMessage,
  ToolMessage,
} from '@langchain/core/messages';
import type { RunnableConfig } from '@langchain/core/runnables';
import type { UsageMetadata } from '@langchain/core/messages';
import type * as t from '@/types';
import {
  repairOrphanedToolMessages,
  createPruneMessages,
  preFlightTruncateToolCallInputs,
  sanitizeOrphanToolBlocks,
  getMessagesWithinTokenLimit as realGetMessagesWithinTokenLimit,
} from '@/messages/prune';
import { getLLMConfig } from '@/utils/llmConfig';
import { Providers, ContentTypes } from '@/common';
import { Run } from '@/run';

// Create a simple token counter for testing
const createTestTokenCounter = (): t.TokenCounter => {
  // This simple token counter just counts characters as tokens for predictable testing
  return (message: BaseMessage): number => {
    // Use type assertion to help TypeScript understand the type
    const content = message.content as
      | string
      | Array<t.MessageContentComplex | string>
      | undefined;

    // Handle string content
    if (typeof content === 'string') {
      return content.length;
    }

    // Handle array content
    if (Array.isArray(content)) {
      let totalLength = 0;

      for (const item of content) {
        if (typeof item === 'string') {
          totalLength += item.length;
        } else if (typeof item === 'object') {
          if ('text' in item && typeof item.text === 'string') {
            totalLength += item.text.length;
          }
          // Count tool_use input fields (serialized args contribute to token count)
          if ('input' in item && item.input != null) {
            const input = item.input;
            totalLength +=
              typeof input === 'string'
                ? input.length
                : JSON.stringify(input).length;
          }
        }
      }

      return totalLength;
    }

    // Default case - if content is null, undefined, or any other type
    return 0;
  };
};

// Since the internal functions in prune.ts are not exported, we'll reimplement them here for testing
// This is based on the implementation in src/messages/prune.ts
function calculateTotalTokens(usage: Partial<UsageMetadata>): UsageMetadata {
  const baseInputTokens = Number(usage.input_tokens) || 0;
  const cacheCreation = Number(usage.input_token_details?.cache_creation) || 0;
  const cacheRead = Number(usage.input_token_details?.cache_read) || 0;

  const totalInputTokens = baseInputTokens + cacheCreation + cacheRead;
  const totalOutputTokens = Number(usage.output_tokens) || 0;

  return {
    input_tokens: totalInputTokens,
    output_tokens: totalOutputTokens,
    total_tokens: totalInputTokens + totalOutputTokens,
  };
}

function getMessagesWithinTokenLimit({
  messages: _messages,
  maxContextTokens,
  indexTokenCountMap,
  startType,
}: {
  messages: BaseMessage[];
  maxContextTokens: number;
  indexTokenCountMap: Record<string, number>;
  startType?: string;
}): {
  context: BaseMessage[];
  remainingContextTokens: number;
  messagesToRefine: BaseMessage[];
  summaryIndex: number;
} {
  // Every reply is primed with <|start|>assistant<|message|>, so we
  // start with 3 tokens for the label after all messages have been counted.
  let summaryIndex = -1;
  let currentTokenCount = 3;
  const instructions =
    _messages[0]?.getType() === 'system' ? _messages[0] : undefined;
  const instructionsTokenCount =
    instructions != null ? indexTokenCountMap[0] : 0;
  let remainingContextTokens = maxContextTokens - instructionsTokenCount;
  const messages = [..._messages];
  const context: BaseMessage[] = [];

  if (currentTokenCount < remainingContextTokens) {
    let currentIndex = messages.length;
    while (
      messages.length > 0 &&
      currentTokenCount < remainingContextTokens &&
      currentIndex > 1
    ) {
      currentIndex--;
      if (messages.length === 1 && instructions) {
        break;
      }
      const poppedMessage = messages.pop();
      if (!poppedMessage) continue;

      const tokenCount = indexTokenCountMap[currentIndex] || 0;

      if (currentTokenCount + tokenCount <= remainingContextTokens) {
        context.push(poppedMessage);
        currentTokenCount += tokenCount;
      } else {
        messages.push(poppedMessage);
        break;
      }
    }

    // If startType is specified, discard messages until we find one of the required type
    if (startType != null && startType && context.length > 0) {
      const requiredTypeIndex = context.findIndex(
        (msg) => msg.getType() === startType
      );

      if (requiredTypeIndex > 0) {
        // If we found a message of the required type, discard all messages before it
        const remainingMessages = context.slice(requiredTypeIndex);
        context.length = 0; // Clear the array
        context.push(...remainingMessages);
      }
    }
  }

  if (instructions && _messages.length > 0) {
    context.push(_messages[0] as BaseMessage);
    messages.shift();
  }

  const prunedMemory = messages;
  summaryIndex = prunedMemory.length - 1;
  remainingContextTokens -= currentTokenCount;

  return {
    summaryIndex,
    remainingContextTokens,
    context: context.reverse(),
    messagesToRefine: prunedMemory,
  };
}

function checkValidNumber(value: unknown): value is number {
  return typeof value === 'number' && !isNaN(value) && value > 0;
}

describe('Prune Messages Tests', () => {
  jest.setTimeout(30000);

  describe('calculateTotalTokens', () => {
    it('should calculate total tokens correctly with all fields present', () => {
      const usage: Partial<UsageMetadata> = {
        input_tokens: 100,
        output_tokens: 50,
        input_token_details: {
          cache_creation: 10,
          cache_read: 5,
        },
      };

      const result = calculateTotalTokens(usage);

      expect(result.input_tokens).toBe(115); // 100 + 10 + 5
      expect(result.output_tokens).toBe(50);
      expect(result.total_tokens).toBe(165); // 115 + 50
    });

    it('should handle missing fields gracefully', () => {
      const usage: Partial<UsageMetadata> = {
        input_tokens: 100,
        output_tokens: 50,
      };

      const result = calculateTotalTokens(usage);

      expect(result.input_tokens).toBe(100);
      expect(result.output_tokens).toBe(50);
      expect(result.total_tokens).toBe(150);
    });

    it('should handle empty usage object', () => {
      const usage: Partial<UsageMetadata> = {};

      const result = calculateTotalTokens(usage);

      expect(result.input_tokens).toBe(0);
      expect(result.output_tokens).toBe(0);
      expect(result.total_tokens).toBe(0);
    });
  });

  describe('getMessagesWithinTokenLimit', () => {
    it('should include all messages when under token limit', () => {
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Hello'),
        new AIMessage('Hi there'),
      ];

      const indexTokenCountMap = {
        0: 17, // "System instruction"
        1: 5, // "Hello"
        2: 8, // "Hi there"
      };

      const result = getMessagesWithinTokenLimit({
        messages,
        maxContextTokens: 100,
        indexTokenCountMap,
      });

      expect(result.context.length).toBe(3);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[0].getType()).toBe('system'); // System message
      expect(result.remainingContextTokens).toBe(100 - 17 - 5 - 8 - 3); // -3 for the assistant label tokens
      expect(result.messagesToRefine.length).toBe(0);
    });

    it('should prune oldest messages when over token limit', () => {
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Message 1'),
        new AIMessage('Response 1'),
        new HumanMessage('Message 2'),
        new AIMessage('Response 2'),
      ];

      const indexTokenCountMap = {
        0: 17, // "System instruction"
        1: 9, // "Message 1"
        2: 10, // "Response 1"
        3: 9, // "Message 2"
        4: 10, // "Response 2"
      };

      // Set a limit that can only fit the system message and the last two messages
      const result = getMessagesWithinTokenLimit({
        messages,
        maxContextTokens: 40,
        indexTokenCountMap,
      });

      // Should include system message and the last two messages
      expect(result.context.length).toBe(3);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[0].getType()).toBe('system'); // System message
      expect(result.context[1]).toBe(messages[3]); // Message 2
      expect(result.context[2]).toBe(messages[4]); // Response 2

      // Should have the first two messages in messagesToRefine
      expect(result.messagesToRefine.length).toBe(2);
      expect(result.messagesToRefine[0]).toBe(messages[1]); // Message 1
      expect(result.messagesToRefine[1]).toBe(messages[2]); // Response 1
    });

    it('should always include system message even when at token limit', () => {
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Hello'),
        new AIMessage('Hi there'),
      ];

      const indexTokenCountMap = {
        0: 17, // "System instruction"
        1: 5, // "Hello"
        2: 8, // "Hi there"
      };

      // Set a limit that can only fit the system message
      const result = getMessagesWithinTokenLimit({
        messages,
        maxContextTokens: 20,
        indexTokenCountMap,
      });

      expect(result.context.length).toBe(1);
      expect(result.context[0]).toBe(messages[0]); // System message

      expect(result.messagesToRefine.length).toBe(2);
    });

    it('should start context with a specific message type when startType is specified', () => {
      const messages = [
        new SystemMessage('System instruction'),
        new AIMessage('AI message 1'),
        new HumanMessage('Human message 1'),
        new AIMessage('AI message 2'),
        new HumanMessage('Human message 2'),
      ];

      const indexTokenCountMap = {
        0: 17, // "System instruction"
        1: 12, // "AI message 1"
        2: 15, // "Human message 1"
        3: 12, // "AI message 2"
        4: 15, // "Human message 2"
      };

      // Set a limit that can fit all messages
      const result = getMessagesWithinTokenLimit({
        messages,
        maxContextTokens: 100,
        indexTokenCountMap,
        startType: 'human',
      });

      // All messages should be included since we're under the token limit
      expect(result.context.length).toBe(5);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[1]); // AI message 1
      expect(result.context[2]).toBe(messages[2]); // Human message 1
      expect(result.context[3]).toBe(messages[3]); // AI message 2
      expect(result.context[4]).toBe(messages[4]); // Human message 2

      // All messages should be included since we're under the token limit
      expect(result.messagesToRefine.length).toBe(0);
    });

    it('should keep all messages if no message of required type is found', () => {
      const messages = [
        new SystemMessage('System instruction'),
        new AIMessage('AI message 1'),
        new AIMessage('AI message 2'),
      ];

      const indexTokenCountMap = {
        0: 17, // "System instruction"
        1: 12, // "AI message 1"
        2: 12, // "AI message 2"
      };

      // Set a limit that can fit all messages
      const result = getMessagesWithinTokenLimit({
        messages,
        maxContextTokens: 100,
        indexTokenCountMap,
        startType: 'human',
      });

      // Should include all messages since no human messages exist to start from
      expect(result.context.length).toBe(3);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[1]); // AI message 1
      expect(result.context[2]).toBe(messages[2]); // AI message 2

      expect(result.messagesToRefine.length).toBe(0);
    });
  });

  describe('checkValidNumber', () => {
    it('should return true for valid positive numbers', () => {
      expect(checkValidNumber(5)).toBe(true);
      expect(checkValidNumber(1.5)).toBe(true);
      expect(checkValidNumber(Number.MAX_SAFE_INTEGER)).toBe(true);
    });

    it('should return false for zero, negative numbers, and NaN', () => {
      expect(checkValidNumber(0)).toBe(false);
      expect(checkValidNumber(-5)).toBe(false);
      expect(checkValidNumber(NaN)).toBe(false);
    });

    it('should return false for non-number types', () => {
      expect(checkValidNumber('5')).toBe(false);
      expect(checkValidNumber(null)).toBe(false);
      expect(checkValidNumber(undefined)).toBe(false);
      expect(checkValidNumber({})).toBe(false);
      expect(checkValidNumber([])).toBe(false);
    });
  });

  describe('createPruneMessages', () => {
    it('should return all messages when under token limit', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Hello'),
        new AIMessage('Hi there'),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
        2: tokenCounter(messages[2]),
      };

      const pruneMessages = createPruneMessages({
        maxTokens: 100,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
      });

      const result = pruneMessages({ messages });

      expect(result.context.length).toBe(3);
      expect(result.context).toEqual(messages);
      expect(result.messagesToRefine).toEqual([]);
      expect(result.remainingContextTokens).toBeGreaterThan(0);
    });

    it('should prune messages when over token limit', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Message 1'),
        new AIMessage('Response 1'),
        new HumanMessage('Message 2'),
        new AIMessage('Response 2'),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
        2: tokenCounter(messages[2]),
        3: tokenCounter(messages[3]),
        4: tokenCounter(messages[4]),
      };

      // Set a limit that can only fit the system message and the last two messages
      const pruneMessages = createPruneMessages({
        maxTokens: 40,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
      });

      const result = pruneMessages({ messages });

      // Should include system message and the last two messages
      expect(result.context.length).toBe(3);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[3]); // Message 2
      expect(result.context[2]).toBe(messages[4]); // Response 2
      expect(Array.isArray(result.messagesToRefine)).toBe(true);
      expect(result.messagesToRefine?.length).toBe(2);
      expect(typeof result.remainingContextTokens).toBe('number');
    });

    it('should respect startType parameter', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new AIMessage('AI message 1'),
        new HumanMessage('Human message 1'),
        new AIMessage('AI message 2'),
        new HumanMessage('Human message 2'),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
        2: tokenCounter(messages[2]),
        3: tokenCounter(messages[3]),
        4: tokenCounter(messages[4]),
      };

      // Set a limit that can fit all messages
      const pruneMessages = createPruneMessages({
        maxTokens: 100,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      const result = pruneMessages({
        messages,
        startType: 'human',
      });

      // All messages should be included since we're under the token limit
      expect(result.context.length).toBe(5);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[1]); // AI message 1
      expect(result.context[2]).toBe(messages[2]); // Human message 1
      expect(result.context[3]).toBe(messages[3]); // AI message 2
      expect(result.context[4]).toBe(messages[4]); // Human message 2
    });

    it('should update token counts when usage metadata is provided', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Hello'),
        new AIMessage('Hi there'),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
        2: tokenCounter(messages[2]),
      };

      const pruneMessages = createPruneMessages({
        maxTokens: 100,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      // Provide usage metadata that indicates different token counts
      const usageMetadata: Partial<UsageMetadata> = {
        input_tokens: 50,
        output_tokens: 25,
        total_tokens: 75,
      };

      const result = pruneMessages({
        messages,
        usageMetadata,
      });

      // The function should have updated the indexTokenCountMap based on the usage metadata
      expect(result.indexTokenCountMap).not.toEqual(indexTokenCountMap);

      // The total of all values in indexTokenCountMap should equal the total_tokens from usageMetadata
      const totalTokens = Object.values(result.indexTokenCountMap).reduce(
        (a = 0, b = 0) => a + b,
        0
      );
      expect(totalTokens).toBe(75);
    });
  });

  describe('Tool Message Handling', () => {
    it('should drop orphan tool messages that no longer have matching AI tool calls', () => {
      const tokenCounter = createTestTokenCounter();
      const context = [
        new SystemMessage('System instruction'),
        new ToolMessage({
          content: 'Orphan result',
          tool_call_id: 'tool-orphan',
        }),
        new AIMessage({
          content: [
            { type: 'text', text: 'I will call a tool now' },
            {
              type: 'tool_use',
              id: 'tool-valid',
              name: 'read_file',
              input: '{"path":"README.md"}',
            },
          ],
        }),
        new ToolMessage({
          content: 'Valid result',
          tool_call_id: 'tool-valid',
        }),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(context[0]),
        1: tokenCounter(context[1]),
        2: tokenCounter(context[2]),
        3: tokenCounter(context[3]),
      };

      const repaired = repairOrphanedToolMessages({
        context,
        allMessages: context,
        tokenCounter,
        indexTokenCountMap,
      });

      expect(repaired.context).toHaveLength(3);
      expect(repaired.context[0]).toBe(context[0]);
      expect(repaired.context[1]).toBe(context[2]);
      expect(repaired.context[2]).toBe(context[3]);
      expect(repaired.droppedOrphanCount).toBe(1);
      expect(repaired.reclaimedTokens).toBe(indexTokenCountMap[1]);
    });

    it('should strip orphan tool_use blocks from AI messages when ToolMessages are not in context', () => {
      const tokenCounter = createTestTokenCounter();
      const context = [
        new HumanMessage('Show me something cool'),
        new AIMessage({
          content: [
            { type: 'text', text: 'Let me create an animation.' },
            {
              type: 'tool_use',
              id: 'tool-navigate',
              name: 'navigate_page',
              input: '{"url":"about:blank"}',
            },
            {
              type: 'tool_use',
              id: 'tool-script',
              name: 'evaluate_script',
              input: '{"function":"' + 'x'.repeat(3000) + '"}',
            },
          ],
          tool_calls: [
            {
              id: 'tool-navigate',
              name: 'navigate_page',
              args: { url: 'about:blank' },
            },
            {
              id: 'tool-script',
              name: 'evaluate_script',
              args: { fn: 'x'.repeat(3000) },
            },
          ],
        }),
        // ToolMessages for both tool calls are NOT in context (pruned)
      ];

      const indexTokenCountMap = {
        0: tokenCounter(context[0]),
        1: tokenCounter(context[1]),
      };

      const repaired = repairOrphanedToolMessages({
        context,
        allMessages: context,
        tokenCounter,
        indexTokenCountMap,
      });

      // AI message should survive but with tool_use blocks stripped
      expect(repaired.context).toHaveLength(2);
      const repairedAI = repaired.context[1] as AIMessage;
      expect(repairedAI.getType()).toBe('ai');

      // Should only have the text block, no tool_use blocks
      const content = repairedAI.content as Array<{ type: string }>;
      expect(content).toHaveLength(1);
      expect(content[0].type).toBe('text');

      // tool_calls should be empty
      expect(repairedAI.tool_calls ?? []).toHaveLength(0);

      // Token savings from stripping the large tool_use blocks
      expect(repaired.reclaimedTokens).toBeGreaterThan(0);
    });

    it('should drop AI message entirely when it has only tool_use blocks with no text', () => {
      const tokenCounter = createTestTokenCounter();
      const context = [
        new HumanMessage('Do something'),
        new AIMessage({
          content: [
            {
              type: 'tool_use',
              id: 'tool-only',
              name: 'some_tool',
              input: '{"query":"test"}',
            },
          ],
          tool_calls: [
            { id: 'tool-only', name: 'some_tool', args: { query: 'test' } },
          ],
        }),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(context[0]),
        1: tokenCounter(context[1]),
      };

      const repaired = repairOrphanedToolMessages({
        context,
        allMessages: context,
        tokenCounter,
        indexTokenCountMap,
      });

      // AI message should be dropped since it had only tool_use blocks
      expect(repaired.context).toHaveLength(1);
      expect(repaired.context[0].getType()).toBe('human');
      expect(repaired.droppedOrphanCount).toBe(1);
    });

    it('should keep tool_use blocks when their ToolMessages ARE in context', () => {
      const tokenCounter = createTestTokenCounter();
      const context = [
        new HumanMessage('Do something'),
        new AIMessage({
          content: [
            { type: 'text', text: 'Calling tool' },
            {
              type: 'tool_use',
              id: 'tool-present',
              name: 'read_file',
              input: '{"path":"test.txt"}',
            },
          ],
          tool_calls: [
            {
              id: 'tool-present',
              name: 'read_file',
              args: { path: 'test.txt' },
            },
          ],
        }),
        new ToolMessage({
          content: 'File contents here',
          tool_call_id: 'tool-present',
        }),
      ];

      const indexTokenCountMap = {
        0: tokenCounter(context[0]),
        1: tokenCounter(context[1]),
        2: tokenCounter(context[2]),
      };

      const repaired = repairOrphanedToolMessages({
        context,
        allMessages: context,
        tokenCounter,
        indexTokenCountMap,
      });

      // Nothing should change — all tool_use blocks have matching ToolMessages
      expect(repaired.context).toHaveLength(3);
      expect(repaired.reclaimedTokens).toBe(0);
      expect(repaired.droppedOrphanCount).toBe(0);
    });

    it('should ensure context does not start with a tool message by finding an AI message', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new AIMessage({
          content: [{ type: 'text', text: 'AI msg 1' }],
          tool_calls: [{ id: 'tool1', name: 'read_file', args: { p: '1' } }],
        }),
        new ToolMessage({ content: 'Tool result 1', tool_call_id: 'tool1' }),
        new AIMessage({
          content: [{ type: 'text', text: 'AI msg 2' }],
          tool_calls: [{ id: 'tool2', name: 'read_file', args: { p: '2' } }],
        }),
        new ToolMessage({ content: 'Tool result 2', tool_call_id: 'tool2' }),
      ];

      const indexTokenCountMap = {
        0: 17, // System instruction
        1: 12, // AI message 1
        2: 13, // Tool result 1
        3: 12, // AI message 2
        4: 13, // Tool result 2
      };

      // Create a pruneMessages function with a token limit that will only include the last few messages
      const pruneMessages = createPruneMessages({
        maxTokens: 58, // Only enough for system + last 3 messages + 3, but should not include a parent-less tool message
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      const result = pruneMessages({ messages });

      // The context should include the system message, AI message 2, and Tool result 2
      // AI message 1 + Tool result 1 are pruned. Tool result 1 is orphaned (AI 1 pruned).
      expect(result.context.length).toBe(3);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1].getType()).toBe('ai'); // AI message 2
      expect(result.context[2]).toBe(messages[4]); // Tool result 2
    });

    it('should ensure context does not start with a tool message by finding a human message', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Human message 1'),
        new AIMessage('AI message 1'),
        new ToolMessage({ content: 'Tool result 1', tool_call_id: 'tool1' }),
        new HumanMessage('Human message 2'),
        // Tool result 2 has no parent AI tool_call — this is an orphan
        new ToolMessage({ content: 'Tool result 2', tool_call_id: 'tool2' }),
      ];

      const indexTokenCountMap = {
        0: 17, // System instruction
        1: 15, // Human message 1
        2: 12, // AI message 1
        3: 13, // Tool result 1
        4: 15, // Human message 2
        5: 13, // Tool result 2
      };

      // Create a pruneMessages function with a token limit that will only include the last few messages
      const pruneMessages = createPruneMessages({
        maxTokens: 48, // Only enough for system + last 2 messages
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      const result = pruneMessages({ messages });

      // Tool result 2 is an orphan (no AI message with tool_call_id 'tool2' in context)
      // so it gets dropped. Context is system + human message 2.
      expect(result.context.length).toBe(2);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[4]); // Human message 2
    });

    it('should handle the case where a tool message is followed by an AI message', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Human message'),
        new AIMessage('AI message with tool use'),
        new ToolMessage({ content: 'Tool result', tool_call_id: 'tool1' }),
        new AIMessage('AI message after tool'),
      ];

      const indexTokenCountMap = {
        0: 17, // System instruction
        1: 13, // Human message
        2: 22, // AI message with tool use
        3: 11, // Tool result
        4: 19, // AI message after tool
      };

      const pruneMessages = createPruneMessages({
        maxTokens: 50,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      const result = pruneMessages({ messages });

      expect(result.context.length).toBe(2);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[4]); // AI message after tool
    });

    it('should handle the case where a tool message is followed by a human message', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Human message 1'),
        new AIMessage('AI message with tool use'),
        new ToolMessage({ content: 'Tool result', tool_call_id: 'tool1' }),
        new HumanMessage('Human message 2'),
      ];

      const indexTokenCountMap = {
        0: 17, // System instruction
        1: 15, // Human message 1
        2: 22, // AI message with tool use
        3: 11, // Tool result
        4: 15, // Human message 2
      };

      const pruneMessages = createPruneMessages({
        maxTokens: 46,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      const result = pruneMessages({ messages });

      expect(result.context.length).toBe(2);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1]).toBe(messages[4]); // Human message 2
    });

    it('should handle complex sequence with multiple tool messages', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System instruction'),
        new HumanMessage('Human message 1'),
        new AIMessage({
          content: [{ type: 'text', text: 'AI message 1' }],
          tool_calls: [{ id: 'tool1', name: 'read_file', args: { path: 'a' } }],
        }),
        new ToolMessage({ content: 'Tool result 1', tool_call_id: 'tool1' }),
        new AIMessage({
          content: [{ type: 'text', text: 'AI message 2' }],
          tool_calls: [{ id: 'tool2', name: 'read_file', args: { path: 'b' } }],
        }),
        new ToolMessage({ content: 'Tool result 2', tool_call_id: 'tool2' }),
        new AIMessage({
          content: [{ type: 'text', text: 'AI message 3' }],
          tool_calls: [{ id: 'tool3', name: 'read_file', args: { path: 'c' } }],
        }),
        new ToolMessage({ content: 'Tool result 3', tool_call_id: 'tool3' }),
      ];

      const indexTokenCountMap = {
        0: 17, // System instruction
        1: 15, // Human message 1
        2: 26, // AI message 1 with tool use
        3: 13, // Tool result 1
        4: 26, // AI message 2 with tool use
        5: 13, // Tool result 2
        6: 26, // AI message 3 with tool use
        7: 13, // Tool result 3
      };

      const pruneMessages = createPruneMessages({
        maxTokens: 111,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: { ...indexTokenCountMap },
      });

      const result = pruneMessages({ messages });

      // AI messages 2 & 3 with their ToolMessages fit; AI1+Tool1 pruned; Tool1 is orphan (AI1 pruned)
      expect(result.context.length).toBe(5);
      expect(result.context[0]).toBe(messages[0]); // System message
      expect(result.context[1].getType()).toBe('ai'); // AI message 2
      expect(result.context[2]).toBe(messages[5]); // Tool result 2
      expect(result.context[3].getType()).toBe('ai'); // AI message 3
      expect(result.context[4]).toBe(messages[7]); // Tool result 3
    });
  });

  describe('preFlightTruncateToolCallInputs', () => {
    it('should truncate oversized tool_use input fields in AI messages', () => {
      const tokenCounter = createTestTokenCounter();
      const largeInput = '{"function":"' + 'x'.repeat(5000) + '"}';
      const messages: BaseMessage[] = [
        new HumanMessage('Run this script'),
        new AIMessage({
          content: [
            { type: 'text', text: 'I will execute the script.' },
            {
              type: 'tool_use',
              id: 'tool-exec',
              name: 'evaluate_script',
              input: largeInput,
            },
          ],
          tool_calls: [
            {
              id: 'tool-exec',
              name: 'evaluate_script',
              args: { function: 'x'.repeat(5000) },
            },
          ],
        }),
        new ToolMessage({ content: 'Result: OK', tool_call_id: 'tool-exec' }),
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
        2: tokenCounter(messages[2]),
      };

      const originalTokens = indexTokenCountMap[1] as number;
      expect(originalTokens).toBeGreaterThan(5000); // Large input counted

      // maxContextTokens: 1000 → maxInputChars = floor(1000 * 0.15) * 4 = 600
      const truncated = preFlightTruncateToolCallInputs({
        messages,
        maxContextTokens: 1000,
        indexTokenCountMap,
        tokenCounter,
      });

      expect(truncated).toBe(1);
      const newTokens = indexTokenCountMap[1] as number;
      expect(newTokens).toBeLessThan(originalTokens);

      // Verify the content block was truncated
      const aiMsg = messages[1] as AIMessage;
      const toolUseBlock = (
        aiMsg.content as Array<Record<string, unknown>>
      ).find((b) => b.type === 'tool_use');
      expect(toolUseBlock).toBeDefined();
      const truncatedInput = toolUseBlock!.input as {
        _truncated: string;
        _originalChars: number;
      };
      expect(truncatedInput._truncated).toContain('[truncated:');
      expect(truncatedInput._originalChars).toBeGreaterThan(600);

      // Verify tool_calls args were also truncated
      expect(aiMsg.tool_calls).toBeDefined();
      const tc = aiMsg.tool_calls![0];
      expect(tc.args).toHaveProperty('_truncated');
    });

    it('should not truncate inputs that fit within the budget', () => {
      const tokenCounter = createTestTokenCounter();
      const messages: BaseMessage[] = [
        new HumanMessage('Read a file'),
        new AIMessage({
          content: [
            { type: 'text', text: 'Reading file.' },
            {
              type: 'tool_use',
              id: 'tool-read',
              name: 'read_file',
              input: '{"path":"test.txt"}',
            },
          ],
          tool_calls: [
            { id: 'tool-read', name: 'read_file', args: { path: 'test.txt' } },
          ],
        }),
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
      };

      const originalTokens = indexTokenCountMap[1];

      const truncated = preFlightTruncateToolCallInputs({
        messages,
        maxContextTokens: 1000,
        indexTokenCountMap,
        tokenCounter,
      });

      expect(truncated).toBe(0);
      expect(indexTokenCountMap[1]).toBe(originalTokens);
    });

    it('should skip non-AI messages', () => {
      const tokenCounter = createTestTokenCounter();
      const messages: BaseMessage[] = [
        new HumanMessage('Hello'),
        new ToolMessage({ content: 'x'.repeat(5000), tool_call_id: 'tool-1' }),
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {
        0: tokenCounter(messages[0]),
        1: tokenCounter(messages[1]),
      };

      const truncated = preFlightTruncateToolCallInputs({
        messages,
        maxContextTokens: 1000,
        indexTokenCountMap,
        tokenCounter,
      });

      // Should not touch ToolMessages (that's preFlightTruncateToolResults' job)
      expect(truncated).toBe(0);
    });
  });

  describe('Instruction token budget reservation (getInstructionTokens)', () => {
    it('should reserve budget for instruction tokens when no system message is present', () => {
      const tokenCounter = createTestTokenCounter();
      // Agent flow: messages do NOT include a system message.
      // The system message is prepended later by buildSystemRunnable.
      const messages = [
        new HumanMessage('Hello there'), // 11 chars
        new AIMessage('Hi'), // 2 chars
        new HumanMessage('How are you?'), // 12 chars
        new AIMessage('Good'), // 4 chars
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      // Total message tokens: 11 + 2 + 12 + 4 = 29
      // Instruction tokens: 20 (simulating system prompt overhead)
      // Effective budget for messages: 50 - 20 = 30 → fits all 29 tokens
      const pruneMessages = createPruneMessages({
        maxTokens: 50,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => 20,
      });

      const result = pruneMessages({ messages });

      // All messages should fit: 29 message tokens + 20 instruction = 49 ≤ 50
      expect(result.context.length).toBe(4);
      expect(result.context).toEqual(messages);
      expect(result.messagesToRefine).toEqual([]);
    });

    it('should prune when messages + instruction tokens exceed budget', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new HumanMessage('Hello there'), // 11 chars
        new AIMessage('Hi'), // 2 chars
        new HumanMessage('How are you?'), // 12 chars
        new AIMessage('Good'), // 4 chars
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      // Total message tokens: 29
      // Instruction tokens: 25 (simulating large tool schema overhead)
      // Effective budget: 40 - 25 = 15 → must prune older messages
      const pruneMessages = createPruneMessages({
        maxTokens: 40,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => 25,
      });

      const result = pruneMessages({ messages });

      // Should prune older messages to fit within 15 available tokens.
      // Working backwards: "Good" (4) + "How are you?" (12) = 16 > 15
      // So only "Good" (4) fits, context starts on that AI message.
      // But startType may require a human message...
      // Actually with no startType and 3 tokens of overhead,
      // available = 15 - 3 = 12: "Good" (4) fits, "How are you?" (12) → 4+12=16 > 12
      // So only "Good" (4) fits.
      expect(result.context.length).toBeLessThan(4);
      expect(Array.isArray(result.messagesToRefine)).toBe(true);
      expect(result.messagesToRefine!.length).toBeGreaterThan(0);
    });

    it('should correctly account for instruction tokens in early-return path', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new HumanMessage('Hi'), // 2 chars
        new AIMessage('Hello'), // 5 chars
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      // Message tokens: 7
      // Instruction tokens: 100 (simulating 26 MCP tools ~5000 chars)
      // Budget: 50 → 7 + 100 = 107 > 50, so early-return should NOT fire
      const pruneMessages = createPruneMessages({
        maxTokens: 50,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => 100,
      });

      const result = pruneMessages({ messages });

      // Even though messages alone (7) fit in 50, the instruction overhead (100)
      // means pruning must occur.  With only 50 - 100 = -50 effective budget,
      // nothing fits → all messages pruned.
      expect(result.messagesToRefine!.length).toBeGreaterThan(0);
    });

    it('should not double-subtract when messages include a system message', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new SystemMessage('System'), // 6 chars
        new HumanMessage('Hello there'), // 11 chars
        new AIMessage('Hi'), // 2 chars
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      // When messages[0] IS a system message, getMessagesWithinTokenLimit uses
      // indexTokenCountMap[0] (6) to subtract from budget, ignoring instructionTokens.
      // getInstructionTokens is only used when no system message is at index 0.
      const pruneMessages = createPruneMessages({
        maxTokens: 30,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => 999, // Should be ignored for system message path
      });

      const result = pruneMessages({ messages });

      // Budget: 30 - 6 (system) = 24 available.
      // "Hi" (2) + "Hello there" (11) + 3 overhead = 16, fits in 24.
      // All messages should be kept.
      expect(result.context.length).toBe(3);
      expect(result.context[0]).toBe(messages[0]); // System message preserved
    });

    it('index 0 should NOT be inflated when getInstructionTokens is provided', () => {
      const tokenCounter = createTestTokenCounter();
      const messages = [
        new HumanMessage('Hello there'), // 11 chars
        new AIMessage('Hi'), // 2 chars
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      const pruneMessages = createPruneMessages({
        maxTokens: 50,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => 10,
      });

      // Before and after pruning, index 0 should remain 11 (the real token count)
      expect(indexTokenCountMap[0]).toBe(11);
      pruneMessages({ messages });
      // The returned indexTokenCountMap should still have the real count at index 0
      expect(indexTokenCountMap[0]).toBe(11);
    });

    it('pre-flight truncation uses effective budget after instruction overhead', () => {
      const tokenCounter = createTestTokenCounter();
      // Simulate the real scenario: AI message has a massive tool_call input
      // (like the chrome-devtools evaluate_script with a 7000-char JS payload)
      const hugeInput = 'x'.repeat(7000);
      const messages = [
        new HumanMessage('show me something'), // 17 chars
        new AIMessage({
          content: [
            { type: 'text', text: 'Creating animation' },
            {
              type: 'tool_use',
              id: 'tool_1',
              name: 'evaluate_script',
              input: { function: hugeInput },
            },
          ],
          tool_calls: [
            {
              id: 'tool_1',
              name: 'evaluate_script',
              args: { function: hugeInput },
              type: 'tool_call' as const,
            },
          ],
        }),
        new ToolMessage({ content: 'Script executed', tool_call_id: 'tool_1' }),
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      // Without instruction overhead: maxTokens=8000, truncation threshold would be
      // Math.floor(8000*0.15)*4 = 4800 chars. The AI message would be ~4800+ tokens
      // (truncated input) which exceeds the real available budget (3600).
      //
      // With instruction overhead: effectiveMax=3600, threshold =
      // Math.floor(3600*0.15)*4 = 2160 chars. The AI message shrinks to ~2200 tokens
      // which fits in the 3600 available budget.
      const instructionTokens = 4400;
      const pruneMessages = createPruneMessages({
        maxTokens: 8000,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => instructionTokens,
      });

      const result = pruneMessages({ messages });

      // The AI message should survive pruning (not be in messagesToRefine)
      // because pre-flight truncation used the effective budget
      const aiMessagesInContext = result.context.filter(
        (m) => m.getType() === 'ai'
      );
      expect(aiMessagesInContext.length).toBe(1);
      expect(result.context.length).toBe(3); // All 3 messages fit after truncation
    });

    it('emergency truncation recovers when initial prune produces empty context', () => {
      const tokenCounter = createTestTokenCounter();
      // Simulate post-summarization state: only 4 messages remain, but one
      // has a huge tool_call input that exceeds available budget alone.
      // With char-based counter, the AI message with 4000-char input is ~4000 tokens.
      // Available budget: 5000 - 4500 = 500. Nothing fits on first pass.
      const hugeInput = 'x'.repeat(4000);
      const messages = [
        new AIMessage({
          content: [
            { type: 'text', text: 'Running script' },
            {
              type: 'tool_use',
              id: 'tool_1',
              name: 'evaluate_script',
              input: { function: hugeInput },
            },
          ],
          tool_calls: [
            {
              id: 'tool_1',
              name: 'evaluate_script',
              args: { function: hugeInput },
              type: 'tool_call' as const,
            },
          ],
        }),
        new ToolMessage({
          content: 'Script ran successfully',
          tool_call_id: 'tool_1',
        }),
        new HumanMessage('that looks great'),
        new AIMessage('Thanks! Want more?'),
      ];

      const indexTokenCountMap: Record<string, number | undefined> = {};
      for (let i = 0; i < messages.length; i++) {
        indexTokenCountMap[i] = tokenCounter(messages[i]);
      }

      // Available budget is extremely tight: 500 tokens for messages.
      // The AI message alone is ~4000+ tokens. Initial prune: nothing fits.
      // Emergency truncation should reduce tool inputs to 150 chars,
      // making the AI message fit.
      const pruneMessages = createPruneMessages({
        maxTokens: 5000,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap,
        getInstructionTokens: () => 4500,
      });

      const result = pruneMessages({ messages });

      // Emergency truncation should have recovered — context is NOT empty
      expect(result.context.length).toBeGreaterThan(0);
      // At minimum, the newest messages should be present
      const types = result.context.map((m) => m.getType());
      expect(types).toContain('human');
    });
  });

  describe('Empty messages guard', () => {
    it('returns empty context without crashing when messages array is empty', () => {
      const tokenCounter = createTestTokenCounter();
      const pruneMessages = createPruneMessages({
        maxTokens: 8000,
        startIndex: 0,
        tokenCounter,
        indexTokenCountMap: {},
        getInstructionTokens: () => 4000,
      });

      // Simulate post-summarization state where REMOVE_ALL left an empty messages array
      const result = pruneMessages({
        messages: [],
        usageMetadata: {
          input_tokens: 100,
          output_tokens: 50,
          total_tokens: 150,
        } as UsageMetadata,
      });

      expect(result.context).toEqual([]);
      expect(result.messagesToRefine).toEqual([]);
      expect(result.prePruneTotalTokens).toBe(0);
      expect(result.remainingContextTokens).toBe(8000);
    });
  });

  describe('Integration with Run', () => {
    it('should initialize Run with custom token counter and process messages', async () => {
      const provider = Providers.OPENAI;
      const llmConfig = getLLMConfig(provider);
      const tokenCounter = createTestTokenCounter();

      const run = await Run.create<t.IState>({
        runId: 'test-prune-run',
        graphConfig: {
          type: 'standard',
          llmConfig,
          instructions: 'You are a helpful assistant.',
          maxContextTokens: 1000,
        },
        returnContent: true,
        tokenCounter,
        indexTokenCountMap: {},
      });

      // Override the model to use a fake LLM
      run.Graph?.overrideTestModel(['This is a test response'], 1);

      const messages = [new HumanMessage('Hello, how are you?')];

      const config: Partial<RunnableConfig> & {
        version: 'v1' | 'v2';
        streamMode: string;
      } = {
        configurable: {
          thread_id: 'test-thread',
        },
        streamMode: 'values',
        version: 'v2' as const,
      };

      await run.processStream({ messages }, config);

      const finalMessages = run.getRunMessages();
      expect(finalMessages).toBeDefined();
      expect(finalMessages?.length).toBeGreaterThan(0);
    });
  });
});

describe('sanitizeOrphanToolBlocks', () => {
  it('strips orphan tool_use blocks from AI messages with no matching ToolMessage', () => {
    const messages: BaseMessage[] = [
      new HumanMessage('Hello'),
      new AIMessage({
        content: [
          { type: 'text', text: 'Let me check.' },
          { type: 'tool_use', id: 'tool_1', name: 'calc', input: { x: 1 } },
        ],
        tool_calls: [
          { id: 'tool_1', name: 'calc', args: { x: 1 }, type: 'tool_call' },
        ],
      }),
      // No ToolMessage for tool_1 — orphan
    ];

    const result = sanitizeOrphanToolBlocks(messages);
    // The stripped AI message was the last message → dropped (incomplete tool call)
    expect(result).toHaveLength(1);
    expect(result[0].getType()).toBe('human');
  });

  it('drops orphan ToolMessages whose AI message is missing', () => {
    const messages: BaseMessage[] = [
      new HumanMessage('Hello'),
      new ToolMessage({
        content: 'result',
        tool_call_id: 'tool_orphan',
        name: 'calc',
      }),
      new AIMessage('Some response'),
    ];

    const result = sanitizeOrphanToolBlocks(messages);
    expect(result).toHaveLength(2); // HumanMessage + AIMessage, orphan ToolMessage dropped
    expect(result[0].getType()).toBe('human');
    expect(result[1].getType()).toBe('ai');
  });

  it('preserves correctly paired tool_use and ToolMessages', () => {
    const messages: BaseMessage[] = [
      new HumanMessage('Compute 1+1'),
      new AIMessage({
        content: [
          { type: 'text', text: 'Let me calculate.' },
          { type: 'tool_use', id: 'tool_a', name: 'calc', input: { x: 1 } },
        ],
        tool_calls: [
          { id: 'tool_a', name: 'calc', args: { x: 1 }, type: 'tool_call' },
        ],
      }),
      new ToolMessage({
        content: '2',
        tool_call_id: 'tool_a',
        name: 'calc',
      }),
      new AIMessage('The answer is 2.'),
    ];

    const result = sanitizeOrphanToolBlocks(messages);
    expect(result).toHaveLength(4); // All messages preserved
    expect(result.map((m) => m.getType())).toEqual([
      'human',
      'ai',
      'tool',
      'ai',
    ]);
  });

  it('drops AI message entirely when it only contained orphan tool_use blocks', () => {
    const messages: BaseMessage[] = [
      new HumanMessage('Do something'),
      new AIMessage({
        content: [{ type: 'tool_use', id: 'tool_x', name: 'run', input: {} }],
        tool_calls: [
          { id: 'tool_x', name: 'run', args: {}, type: 'tool_call' },
        ],
      }),
      // No ToolMessage for tool_x
    ];

    const result = sanitizeOrphanToolBlocks(messages);
    // The AI message had only tool_use blocks, stripping them leaves nothing → dropped
    expect(result).toHaveLength(1);
    expect(result[0].getType()).toBe('human');
  });

  it('keeps stripped AI message in the middle but drops stripped trailing AI', () => {
    const messages: BaseMessage[] = [
      new HumanMessage('First question'),
      new AIMessage({
        content: [
          { type: 'text', text: 'Let me use two tools.' },
          { type: 'tool_use', id: 'tool_a', name: 'calc', input: { x: 1 } },
          {
            type: 'tool_use',
            id: 'tool_orphan',
            name: 'search',
            input: { q: 'test' },
          },
        ],
        tool_calls: [
          { id: 'tool_a', name: 'calc', args: { x: 1 }, type: 'tool_call' },
          {
            id: 'tool_orphan',
            name: 'search',
            args: { q: 'test' },
            type: 'tool_call',
          },
        ],
      }),
      new ToolMessage({
        content: '42',
        tool_call_id: 'tool_a',
        name: 'calc',
      }),
      // No ToolMessage for tool_orphan, but conversation continues:
      new AIMessage({
        content: [{ type: 'text', text: 'Got the calc result.' }],
        tool_calls: [
          { id: 'tool_b', name: 'run', args: {}, type: 'tool_call' },
        ],
      }),
      // tool_b is also orphan → stripped, and this AI is last → dropped
    ];

    const result = sanitizeOrphanToolBlocks(messages);
    // message[1]: AI has tool_orphan stripped but tool_a kept → stays (middle, not trailing)
    // message[3]: AI has tool_b stripped, is trailing → dropped
    expect(result).toHaveLength(3); // HumanMessage, stripped AI (kept tool_a), ToolMessage
    const ai = result[1] as AIMessage;
    expect(ai.tool_calls).toHaveLength(1);
    expect(ai.tool_calls![0].id).toBe('tool_a');
    expect(result[2].getType()).toBe('tool');
  });

  it('keeps unmodified trailing AI message (no orphan tool_use)', () => {
    const messages: BaseMessage[] = [
      new HumanMessage('Hello'),
      new ToolMessage({
        content: 'result',
        tool_call_id: 'tool_orphan',
        name: 'calc',
      }),
      new AIMessage('Final response without tool calls.'),
    ];

    const result = sanitizeOrphanToolBlocks(messages);
    // orphan ToolMessage dropped, trailing AI kept (was not stripped)
    expect(result).toHaveLength(2);
    expect(result[0].getType()).toBe('human');
    expect(result[1].getType()).toBe('ai');
  });

  it('handles plain objects (non-BaseMessage instances) via duck typing', () => {
    // Simulate messages that have lost their class instances (LangGraph state serialization)
    const plainMessages = [
      { role: 'user', content: 'Hello', _type: 'human' },
      {
        role: 'assistant',
        _type: 'ai',
        content: [
          { type: 'text', text: 'Let me check.' },
          { type: 'tool_use', id: 'tool_1', name: 'calc', input: { x: 1 } },
        ],
        tool_calls: [
          { id: 'tool_1', name: 'calc', args: { x: 1 }, type: 'tool_call' },
        ],
      },
      // No ToolMessage for tool_1 — orphan
    ] as unknown as BaseMessage[];

    // Should not throw "getType is not a function"
    const result = sanitizeOrphanToolBlocks(plainMessages);
    // The stripped AI message was the last message → dropped (incomplete tool call)
    expect(result).toHaveLength(1);
  });
});

describe('prunedMemory ordering with thinking enabled', () => {
  it('messagesToRefine preserves chronological order when thinking search pops multiple messages', () => {
    const tokenCounter = createTestTokenCounter();
    const messages: BaseMessage[] = [
      new HumanMessage('Hello'),
      new AIMessage({
        content: [
          {
            type: ContentTypes.REASONING_CONTENT,
            reasoningText: {
              text: 'Thinking about navigation...',
              signature: 'sig1',
            },
          },
          { type: 'text', text: 'Navigating now.' },
        ],
        tool_calls: [
          {
            id: 'tc_nav',
            name: 'navigate',
            args: { url: 'about:blank' },
            type: 'tool_call',
          },
        ],
      }),
      new ToolMessage({
        content: 'Navigated to about:blank.',
        tool_call_id: 'tc_nav',
        name: 'navigate',
      }),
      new AIMessage({
        content: [
          {
            type: ContentTypes.REASONING_CONTENT,
            reasoningText: {
              text: 'Now I will write code...',
              signature: 'sig2',
            },
          },
          { type: 'text', text: 'Running script.' },
        ],
        tool_calls: [
          {
            id: 'tc_eval',
            name: 'evaluate',
            args: { code: 'x'.repeat(5000) },
            type: 'tool_call',
          },
        ],
      }),
      new ToolMessage({
        content: 'y'.repeat(5000), // large tool result
        tool_call_id: 'tc_eval',
        name: 'evaluate',
      }),
    ];

    const indexTokenCountMap: Record<string, number | undefined> = {};
    for (let i = 0; i < messages.length; i++) {
      indexTokenCountMap[i] = tokenCounter(messages[i]);
    }

    // Use a very tight budget so the backward iteration must prune messages
    // The thinking search will cause the loop to `continue` past the large ToolMessage
    const result = realGetMessagesWithinTokenLimit({
      messages,
      maxContextTokens: 200, // very tight
      indexTokenCountMap,
      thinkingEnabled: true,
      tokenCounter,
      reasoningType: ContentTypes.REASONING_CONTENT,
    });

    // The key assertion: messagesToRefine must be in chronological order.
    // AI(evaluate) at index 3 must come BEFORE ToolMessage(evaluate) at index 4.
    for (let i = 0; i < result.messagesToRefine.length - 1; i++) {
      const current = result.messagesToRefine[i];
      const next = result.messagesToRefine[i + 1];
      // A ToolMessage should never come before its AI message
      if (next.getType() === 'ai' && current.getType() === 'tool') {
        const toolId = (current as ToolMessage).tool_call_id;
        const aiToolIds = ((next as AIMessage).tool_calls ?? []).map(
          (tc) => tc.id
        );
        expect(aiToolIds).not.toContain(toolId);
      }
    }

    // Verify the specific ordering: if both AI(evaluate) and Tool(evaluate) are in
    // messagesToRefine, AI must come first.
    const evalAiIdx = result.messagesToRefine.findIndex(
      (m) =>
        m.getType() === 'ai' &&
        ((m as AIMessage).tool_calls ?? []).some((tc) => tc.id === 'tc_eval')
    );
    const evalToolIdx = result.messagesToRefine.findIndex(
      (m) =>
        m.getType() === 'tool' && (m as ToolMessage).tool_call_id === 'tc_eval'
    );
    if (evalAiIdx >= 0 && evalToolIdx >= 0) {
      expect(evalAiIdx).toBeLessThan(evalToolIdx);
    }
  });
});
